[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Same Data, Different Stories: Comparing Visual Explanations in Machine Learning",
    "section": "",
    "text": "1 Introduction\nIn recent decades, the online shopping has played an increasing role in people’s daily lives, the convenience and time saving of it has greatly improved our lifestyle. When we shopping online, an interesting phenomenon is that the recommendation section at the main page keep displaying the products that similar to the products we have previously viewed or searched in the searching engine. Sometimes, as an example of myself, I did not buy it directly but add to cart for later instead, but at other time, I was attracted by the recommended products and buy it immediately. These behaviors is driving firms trying to enhance customer experience and conversion rate to increasingly value knowledge of the elements influencing e-commerce buying behavior. With the machine learning course we have learned this semester, although its algorithms can precisely forecast the user’s behavior, it is also vital to know why a model produce particular predictions.\nIn this project, we intends to mimic an e-commerce environment and investigate how user-level characteristics, such as browsing time, number of categories viewed, number of products in cart, affect the probability of completing a purchase. We not only basically generate the predictive classifier but also evaluate the model interpretability using both linear(Logistic Regression) and non-linear(Random Forest) models. For each model, we create the three different plots to visualize the feature importance and understand the activity of our models. Our plots types are:\n\nAdded Variable Plots\nPartial Dependence Plots\nLIME plots\n\nThese tools allow us to further explore which features are important for two models and how consistent the interpretations are across local and global perspectives. With our shopping behavior scenario, they allow us to gain the information about the features that influence the final purchase behavior and the consistence of influential feature of two models. By perform the comparison between the result of two models, we can also gain information about the accuracy and difference about the two models.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data.html#description",
    "href": "data.html#description",
    "title": "2  Data",
    "section": "2.1 Description",
    "text": "2.1 Description"
  },
  {
    "objectID": "data.html#missing-value-analysis",
    "href": "data.html#missing-value-analysis",
    "title": "2  Data",
    "section": "2.2 Missing value analysis",
    "text": "2.2 Missing value analysis",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "here is some demo change to check for the final project here",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "2  Data",
    "section": "",
    "text": "2.1 Description\nInstead of downloading the dataset from website or from the packages, we replicate a dataset that reflects usual user interactions on an online shopping platform in a controlled and interpretable environment to investigate e-commerce buying behavior. The dataset contains both binary and continuous variables often used in consumer behaviour modelling; each observation relates to one user session. From time spent on the website to interaction history, we developed six explanatory variables and one binary response variable (buy) to capture various facets of a user’s purchasing session. The final purchase result is a binary indicator, it equals 1 if the user bought the product and 0 otherwise.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#feature-explanation",
    "href": "data.html#feature-explanation",
    "title": "2  Data",
    "section": "2.2 Feature Explanation",
    "text": "2.2 Feature Explanation\nSpecifically, the dataset includes the following predictor variables:\n\nbrowsing_time(numeric): Time (in hours) spent browsing the platform during the session.\ncategory_count(integer): Number of different product categories the user interacted with.\nclicked_ad(binary): Whether the user clicked on any product advertisement (1 = yeas, 0 = no).\nitems_in_cart(integer): Number of items added to the shopping cart.\ndevice_type(categorical): Device used for browsing, coded as “desktop” or “mobile”.\nprevious_purchase(binary): Whether the user previously made the purchase on the platform (1 = yeas, 0 = no).\n\nThe response variable is also a binary variable:\n\npurchase(binary): Whether the user complete a purchase during the session (1 = yeas, 0 = no).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#data-simulation",
    "href": "data.html#data-simulation",
    "title": "2  Data",
    "section": "2.3 Data simulation",
    "text": "2.3 Data simulation\nTo simulate these data, we created the six characteristics and the response variable using frequently used statistical distributions to replicate the dataset.\n\nbrowsing_time: drawn from a Normal distribution with a mean = 3 and standard deviation = 1.\ncategory_count: sampled from a Poisson distribution with \\(\\lambda = 4\\).\nclicked_ad: generated from a Bernoulli distribution with a success probability of 0.35(\\(p = 0.35\\)).\nitems_in_cart: sampled from a Poisson distribution with \\(\\lambda = 2\\).\ndevice_type: created as a binary variable with \\(60%\\) probability of being a desktop user and \\(40%\\) being the mobile user.\nprevious_purchase: generated froma Bernoulli distribution with \\(p = 0.25\\).\npurchase: derived using a logistic model with a linear combination of the predictors and additional Gaussian noise for realism.\n\nHere is our simulated dataset:\n\n\nCode\nset.seed(5293)\nn &lt;- 500\n\n# Generate the features for the dataset\nbrowsing_time &lt;- rnorm(n, mean = 3, sd = 1)               # hours\ncategory_count &lt;- rpois(n, lambda = 4)                    # integer count\nclicked_ad &lt;- rbinom(n, 1, prob = 0.35)                   # binary\nitems_in_cart &lt;- rpois(n, lambda = 2)                     # integer count\ndevice_type &lt;- rbinom(n, 1, prob = 0.6)                   # 0 = desktop, 1 = mobile\nprevious_purchases &lt;- rbinom(n, 1, prob = 0.25)           # binary\nnoise &lt;- rnorm(n, 0, 0.5)\n\n# Simulate outcome\nlogit &lt;- -1 + \n  0.8 * browsing_time - \n  0.6 * category_count + \n  1.2 * clicked_ad + \n  0.5 * items_in_cart + \n  0.4 * previous_purchases +\n  0.3 * device_type +\n  noise\n\np &lt;- 1 / (1 + exp(-logit))\npurchase &lt;- rbinom(n, 1, p)\n\n# Assemble data frame\nsim_data &lt;- data.frame(\n  browsing_time,\n  category_count,\n  clicked_ad,\n  items_in_cart,\n  device_type = factor(device_type, levels = c(0, 1), labels = c(\"desktop\", \"mobile\")),\n  previous_purchases,\n  purchase\n)\n\nhead(sim_data)\n\n\n  browsing_time category_count clicked_ad items_in_cart device_type\n1      2.795586              5          1             0     desktop\n2      1.682977              3          1             2      mobile\n3      2.906267              2          1             1      mobile\n4      1.407779              0          0             4     desktop\n5      1.902505              7          1             4     desktop\n6      3.918307              4          0             4      mobile\n  previous_purchases purchase\n1                  0        0\n2                  0        0\n3                  0        1\n4                  0        1\n5                  0        0\n6                  1        1",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "results-linear.html",
    "href": "results-linear.html",
    "title": "3  Linear Model",
    "section": "",
    "text": "3.1 Fit the Linear Model (logistic regression)\nCode\nset.seed(5293)\nn &lt;- 500\n\n# Generate the features for the dataset\nbrowsing_time &lt;- rnorm(n, mean = 3, sd = 1)               # hours\ncategory_count &lt;- rpois(n, lambda = 4)                    # integer count\nclicked_ad &lt;- rbinom(n, 1, prob = 0.35)                   # binary\nitems_in_cart &lt;- rpois(n, lambda = 2)                     # integer count\ndevice_type &lt;- rbinom(n, 1, prob = 0.6)                   # 0 = desktop, 1 = mobile\nprevious_purchases &lt;- rbinom(n, 1, prob = 0.25)           # binary\nnoise &lt;- rnorm(n, 0, 0.5)\n\n# Simulate outcome\nlogit &lt;- -1 + \n  0.8 * browsing_time - \n  0.6 * category_count + \n  1.2 * clicked_ad + \n  0.5 * items_in_cart + \n  0.4 * previous_purchases +\n  0.3 * device_type +\n  noise\n\np &lt;- 1 / (1 + exp(-logit))\npurchase &lt;- rbinom(n, 1, p)\n\n# Assemble data frame\nsim_data &lt;- data.frame(\n  browsing_time,\n  category_count,\n  clicked_ad,\n  items_in_cart,\n  device_type = factor(device_type, levels = c(0, 1), labels = c(\"desktop\", \"mobile\")),\n  previous_purchases,\n  purchase\n)\n\nhead(sim_data)\n\n\n  browsing_time category_count clicked_ad items_in_cart device_type\n1      2.795586              5          1             0     desktop\n2      1.682977              3          1             2      mobile\n3      2.906267              2          1             1      mobile\n4      1.407779              0          0             4     desktop\n5      1.902505              7          1             4     desktop\n6      3.918307              4          0             4      mobile\n  previous_purchases purchase\n1                  0        0\n2                  0        0\n3                  0        1\n4                  0        1\n5                  0        0\n6                  1        1\n\n\nCode\n# fit the logistic regression model\nlog_model &lt;- glm(purchase ~ ., data = sim_data, family = \"binomial\")\n\nsummary(log_model)\n\n\n\nCall:\nglm(formula = purchase ~ ., family = \"binomial\", data = sim_data)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        -1.11467    0.51565  -2.162  0.03064 *  \nbrowsing_time       0.84165    0.12942   6.503 7.85e-11 ***\ncategory_count     -0.72339    0.07769  -9.311  &lt; 2e-16 ***\nclicked_ad          1.61928    0.26507   6.109 1.00e-09 ***\nitems_in_cart       0.58558    0.09632   6.079 1.21e-09 ***\ndevice_typemobile   0.07205    0.24024   0.300  0.76424    \nprevious_purchases  0.76285    0.28375   2.688  0.00718 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 683.31  on 499  degrees of freedom\nResidual deviance: 438.31  on 493  degrees of freedom\nAIC: 452.31\n\nNumber of Fisher Scoring iterations: 5\nBased on the logistic model we generated, the summary result shows that statistically significant predictors include browsing_time, category_count, clicked_ad, items_in_cart, and previous_purchases. This means that these five feature have important impact in purchase behavior. However, device_type is not significant (p = 0.764), indicating that switching from desktop to mobile does not have a strong independent effect on the likelihood of purchase in this model. Signs of coefficients match the logic of the simulation (positive for most predictors, negative for category count), though values differ slightly due to added noise. Additionally, the residual deviance of 438.31, compared to the null deviance of 683.31, suggests that the model explains a substantial portion of the variability in the outcome. By adding these six predictors, the deviance reduced from 683.31 to 438.31, means that our features signigicantly improve the model’s predicting ability. The overall logistic model at this step can be written as:\n\\[\\begin{equation}\n\\log\\left(\\frac{P(\\text{purchase} = 1)}{1 - P(\\text{purchase} = 1)}\\right) =\n-1.1147 + 0.8417 \\cdot \\text{browsing_time}\n- 0.7234 \\cdot \\text{category_count}\n+ 1.6193 \\cdot \\text{clicked_ad}\n+ 0.5856 \\cdot \\text{items_in_cart}\n+ 0.0721 \\cdot \\text{device_type}_{\\text{mobile}}\n+ 0.7629 \\cdot \\text{previous_purchases}\n\\end{equation}\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Model</span>"
    ]
  },
  {
    "objectID": "results-linear.html#added-variable-plots",
    "href": "results-linear.html#added-variable-plots",
    "title": "3  Linear Model",
    "section": "3.2 Added Variable Plots",
    "text": "3.2 Added Variable Plots\nTo further analysis the contribution of each individual predictors, we use the Added Variable Plot (AVP) to more fully considering the influences of other variables. By graphing the response residuals (after regressing out the other predictors) against the predictor residuals (after regressing it out from the other predictors as well), this diagnostic tool shows the partial link between the outcome and a chosen predictor.\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(broom)\n\n# numeric predictors\nnumeric_vars &lt;- sim_data %&gt;%\n  select(where(is.numeric)) %&gt;%\n  select(-purchase) %&gt;%\n  colnames()\n\n# compute AVP residuals and SD of fit per feature\navp_list &lt;- lapply(numeric_vars, function(var) {\n  other_vars &lt;- setdiff(numeric_vars, var)\n  model_y &lt;- glm(as.formula(paste(\"purchase ~\", paste(other_vars, collapse = \" + \"))),\n                 data = sim_data, family = \"binomial\")\n  res_y &lt;- resid(model_y)\n  model_x &lt;- lm(as.formula(paste(var, \"~\", paste(other_vars, collapse = \" + \"))),\n                data = sim_data)\n  res_x &lt;- resid(model_x)\n  avp_fit &lt;- lm(res_y ~ res_x)\n  fit_vals &lt;- predict(avp_fit)\n  sd_fit &lt;- sd(fit_vals)\n  tibble(\n    feature = var,\n    res_x = res_x,\n    res_y = res_y,\n    fit = fit_vals,\n    sd_fit = sd_fit\n  )\n})\n\n# Combine into dataframe\navp_df &lt;- bind_rows(avp_list)\n\n# Order by descending SD\nsd_order &lt;- avp_df %&gt;%\n  group_by(feature) %&gt;%\n  summarise(sd_fit = first(sd_fit)) %&gt;%\n  arrange(desc(sd_fit))\n\navp_df$feature &lt;- factor(avp_df$feature, levels = sd_order$feature)\n\n# Plot \nggplot(avp_df, aes(x = res_x, y = res_y)) +\n  geom_point(alpha = 0.4) +\n  geom_smooth(method = \"lm\", color = \"blue\", linewidth = 0.8) +\n  facet_wrap(~ feature, scales = \"free_x\") +\n  labs(\n    title = \"Added Variable Plots\",\n    x = \"Feature residual (unexplained by other predictors)\",\n    y = \"Purchase residual (unexplained by other predictors)\"\n  ) +\n  theme_minimal()\n\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Model</span>"
    ]
  }
]