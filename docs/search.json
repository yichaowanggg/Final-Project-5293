[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Same Data, Different Stories: Comparing Visual Explanations in Machine Learning",
    "section": "",
    "text": "1 Introduction\nIn recent decades, the online shopping has played an increasing role in people’s daily lives, the convenience and time saving of it has greatly improved our lifestyle. When we shopping online, an interesting phenomenon is that the recommendation section at the main page keep displaying the products that similar to the products we have previously viewed or searched in the searching engine. Sometimes, as an example of myself, I did not buy it directly but add to cart for later instead, but at other time, I was attracted by the recommended products and buy it immediately. These behaviors is driving firms trying to enhance customer experience and conversion rate to increasingly value knowledge of the elements influencing e-commerce buying behavior. With the machine learning course we have learned this semester, although its algorithms can precisely forecast the user’s behavior, it is also vital to know why a model produce particular predictions.\nIn this project, we intends to mimic an e-commerce environment and investigate how user-level characteristics, such as browsing time, number of categories viewed, number of products in cart, affect the probability of completing a purchase. We not only basically generate the predictive classifier but also evaluate the model interpretability using both linear(Logistic Regression) and non-linear(Random Forest) models. For each model, we create the three different plots to visualize the feature importance and understand the activity of our models. Our plots types are:\n\nAdded Variable Plots\nPartial Dependence Plots\nLIME plots\n\nThese tools allow us to further explore which features are important for two models and how consistent the interpretations are across local and global perspectives. With our shopping behavior scenario, they allow us to gain the information about the features that influence the final purchase behavior and the consistence of influential feature of two models. By perform the comparison between the result of two models, we can also gain information about the accuracy and difference about the two models.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data.html#description",
    "href": "data.html#description",
    "title": "2  Data",
    "section": "2.1 Description",
    "text": "2.1 Description"
  },
  {
    "objectID": "data.html#missing-value-analysis",
    "href": "data.html#missing-value-analysis",
    "title": "2  Data",
    "section": "2.2 Missing value analysis",
    "text": "2.2 Missing value analysis",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "here is some demo change to check for the final project here",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "2  Data",
    "section": "",
    "text": "2.1 Description\nInstead of downloading the dataset from website or from the packages, we replicate a dataset that reflects usual user interactions on an online shopping platform in a controlled and interpretable environment to investigate e-commerce buying behavior. The dataset contains both binary and continuous variables often used in consumer behaviour modelling; each observation relates to one user session. From time spent on the website to interaction history, we developed six explanatory variables and one binary response variable (buy) to capture various facets of a user’s purchasing session. The final purchase result is a binary indicator, it equals 1 if the user bought the product and 0 otherwise.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#feature-explanation",
    "href": "data.html#feature-explanation",
    "title": "2  Data",
    "section": "2.2 Feature Explanation",
    "text": "2.2 Feature Explanation\nSpecifically, the dataset includes the following predictor variables:\n\nbrowsing_time(numeric): Time (in hours) spent browsing the platform during the session.\ncategory_count(integer): Number of different product categories the user interacted with.\nclicked_ad(binary): Whether the user clicked on any product advertisement (1 = yeas, 0 = no).\nitems_in_cart(integer): Number of items added to the shopping cart.\ndevice_type(categorical): Device used for browsing, coded as “desktop” or “mobile”.\nprevious_purchase(binary): Whether the user previously made the purchase on the platform (1 = yeas, 0 = no).\n\nThe response variable is also a binary variable:\n\npurchase(binary): Whether the user complete a purchase during the session (1 = yeas, 0 = no).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#data-simulation",
    "href": "data.html#data-simulation",
    "title": "2  Data",
    "section": "2.3 Data simulation",
    "text": "2.3 Data simulation\nTo simulate these data, we created the six characteristics and the response variable using frequently used statistical distributions to replicate the dataset.\n\nbrowsing_time: drawn from a Normal distribution with a mean = 3 and standard deviation = 1.\ncategory_count: sampled from a Poisson distribution with \\(\\lambda = 4\\).\nclicked_ad: generated from a Bernoulli distribution with a success probability of 0.35(\\(p = 0.35\\)).\nitems_in_cart: sampled from a Poisson distribution with \\(\\lambda = 2\\).\ndevice_type: created as a binary variable with \\(60%\\) probability of being a desktop user and \\(40%\\) being the mobile user.\nprevious_purchase: generated froma Bernoulli distribution with \\(p = 0.25\\).\npurchase: derived using a logistic model with a linear combination of the predictors and additional Gaussian noise for realism.\n\nHere is our simulated dataset:\n\n\nCode\nset.seed(5293)\nn &lt;- 500\n\n# Generate the features for the dataset\nbrowsing_time &lt;- rnorm(n, mean = 3, sd = 1)               # hours\ncategory_count &lt;- rpois(n, lambda = 4)                    # integer count\nclicked_ad &lt;- rbinom(n, 1, prob = 0.35)                   # binary\nitems_in_cart &lt;- rpois(n, lambda = 2)                     # integer count\ndevice_type &lt;- rbinom(n, 1, prob = 0.6)                   # 0 = desktop, 1 = mobile\nprevious_purchases &lt;- rbinom(n, 1, prob = 0.25)           # binary\nnoise &lt;- rnorm(n, 0, 0.5)\n\n# Simulate outcome\nlogit &lt;- -1 + \n  0.8 * browsing_time - \n  0.6 * category_count + \n  1.2 * clicked_ad + \n  0.5 * items_in_cart + \n  0.4 * previous_purchases +\n  0.3 * device_type +\n  noise\n\np &lt;- 1 / (1 + exp(-logit))\npurchase &lt;- rbinom(n, 1, p)\n\n# Assemble data frame\nsim_data &lt;- data.frame(\n  browsing_time,\n  category_count,\n  clicked_ad,\n  items_in_cart,\n  device_type = factor(device_type, levels = c(0, 1), labels = c(\"desktop\", \"mobile\")),\n  previous_purchases,\n  purchase\n)\n\nhead(sim_data)\n\n\n  browsing_time category_count clicked_ad items_in_cart device_type\n1      2.795586              5          1             0     desktop\n2      1.682977              3          1             2      mobile\n3      2.906267              2          1             1      mobile\n4      1.407779              0          0             4     desktop\n5      1.902505              7          1             4     desktop\n6      3.918307              4          0             4      mobile\n  previous_purchases purchase\n1                  0        0\n2                  0        0\n3                  0        1\n4                  0        1\n5                  0        0\n6                  1        1",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "results-linear.html",
    "href": "results-linear.html",
    "title": "3  Linear Model",
    "section": "",
    "text": "3.1 Fit the Linear Model (logistic regression)\nCode\nsim_data &lt;- sim_data\n# fit the logistic regression model\nlog_model &lt;- glm(purchase ~ ., data = sim_data, family = \"binomial\")\n\nsummary(log_model)\n\n\n\nCall:\nglm(formula = purchase ~ ., family = \"binomial\", data = sim_data)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        -1.11467    0.51565  -2.162  0.03064 *  \nbrowsing_time       0.84165    0.12942   6.503 7.85e-11 ***\ncategory_count     -0.72339    0.07769  -9.311  &lt; 2e-16 ***\nclicked_ad          1.61928    0.26507   6.109 1.00e-09 ***\nitems_in_cart       0.58558    0.09632   6.079 1.21e-09 ***\ndevice_typemobile   0.07205    0.24024   0.300  0.76424    \nprevious_purchases  0.76285    0.28375   2.688  0.00718 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 683.31  on 499  degrees of freedom\nResidual deviance: 438.31  on 493  degrees of freedom\nAIC: 452.31\n\nNumber of Fisher Scoring iterations: 5\nBased on the logistic model we generated, the summary result shows that statistically significant predictors include browsing_time, category_count, clicked_ad, items_in_cart, and previous_purchases. This means that these five feature have important impact in purchase behavior. However, device_type is not significant (p = 0.764), indicating that switching from desktop to mobile does not have a strong independent effect on the likelihood of purchase in this model. Signs of coefficients match the logic of the simulation (positive for most predictors, negative for category count), though values differ slightly due to added noise. Additionally, the residual deviance of 438.31, compared to the null deviance of 683.31, suggests that the model explains a substantial portion of the variability in the outcome. By adding these six predictors, the deviance reduced from 683.31 to 438.31, means that our features signigicantly improve the model’s predicting ability. The overall logistic model at this step can be written as:\n\\[\\begin{align}\n\\log\\left(\\frac{P(\\text{purchase} = 1)}{1 - P(\\text{purchase} = 1)}\\right) &=\n-1.1147 + 0.8417 \\cdot \\text{browsing\\_time} \\\\\n&\\quad - 0.7234 \\cdot \\text{category\\_count}\n+ 1.6193 \\cdot \\text{clicked\\_ad} \\\\\n&\\quad + 0.5856 \\cdot \\text{items\\_in\\_cart}\n+ 0.0721 \\cdot \\text{device\\_type}_{\\text{mobile}} \\\\\n&\\quad + 0.7629 \\cdot \\text{previous\\_purchases}\n\\end{align}\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Model</span>"
    ]
  },
  {
    "objectID": "results-linear.html#added-variable-plots",
    "href": "results-linear.html#added-variable-plots",
    "title": "3  Linear Model",
    "section": "3.2 Added Variable Plots",
    "text": "3.2 Added Variable Plots\nTo further analysis the contribution of each individual predictors, we use the Added Variable Plot (AVP) to more fully considering the influences of other variables. By graphing the response residuals (after regressing out the other predictors) against the predictor residuals (after regressing it out from the other predictors as well), this diagnostic tool shows the partial link between the outcome and a chosen predictor.\n\n\nCode\nlibrary(tidyverse)\nlibrary(broom)\n\n# numeric predictors\nnumeric_vars &lt;- sim_data %&gt;%\n  select(where(is.numeric)) %&gt;%\n  select(-purchase) %&gt;%\n  colnames()\n\n# compute AVP residuals and SD of fit per feature\navp_list &lt;- lapply(numeric_vars, function(var) {\n  other_vars &lt;- setdiff(numeric_vars, var)\n  model_y &lt;- glm(as.formula(paste(\"purchase ~\", paste(other_vars, collapse = \" + \"))),\n                 data = sim_data, family = \"binomial\")\n  res_y &lt;- resid(model_y)\n  model_x &lt;- lm(as.formula(paste(var, \"~\", paste(other_vars, collapse = \" + \"))),\n                data = sim_data)\n  res_x &lt;- resid(model_x)\n  avp_fit &lt;- lm(res_y ~ res_x)\n  fit_vals &lt;- predict(avp_fit)\n  sd_fit &lt;- sd(fit_vals)\n  tibble(\n    feature = var,\n    res_x = res_x,\n    res_y = res_y,\n    fit = fit_vals,\n    sd_fit = sd_fit\n  )\n})\n\n# Combine into dataframe\navp_df &lt;- bind_rows(avp_list)\n\n# Order by descending SD\nsd_order &lt;- avp_df %&gt;%\n  group_by(feature) %&gt;%\n  summarise(sd_fit = first(sd_fit)) %&gt;%\n  arrange(desc(sd_fit))\n\navp_df$feature &lt;- factor(avp_df$feature, levels = sd_order$feature)\n\n# Plot \nggplot(avp_df, aes(x = res_x, y = res_y)) +\n  geom_point(alpha = 0.4) +\n  geom_smooth(method = \"lm\", color = \"blue\", linewidth = 0.8) +\n  facet_wrap(~ feature, scales = \"free_x\") +\n  labs(\n    title = \"Added Variable Plots\",\n    x = \"Feature residual (unexplained by other predictors)\",\n    y = \"Purchase residual (unexplained by other predictors)\"\n  ) +\n  theme_minimal()\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAfter considering all additional factors in the logistic regression model, the AVPs suggests the marginal influence of each predictor on the binary response variable purchase. Every panel displays the residual of a specified characteristic (also adjusted for other variables) plotted against the residual of purchase (after eliminating the influence of other predictors). A prominent linear trend in the graphic indicates that the related predictor significantly affects the model.\nThe graphs reveal that category_count has a substantial negative linear correlation with the response residuals, suggesting a notable negative link with the likelihood of purchase even after other attributes are controlled. All three variables, browsing_time, clicked_ad, and items_in_cart, show clear positive linear trends consistent with the idea that more browsing, clicks, and cart activity usually raise the probability of purchase. The plot for previous_purchases shows a weaker and less clear trend, suggesting its marginal contribution is smaller, which is consistent with its less significant coefficient in the logistic regression we got above at fitting the model. At last, device_type would fit its high p-value in the regression summary and may not be as important. These graphs show generally where the linear model fits well and help visually verify the significance of the strongest predictors.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Model</span>"
    ]
  },
  {
    "objectID": "results-linear.html#partial-dependence-plots",
    "href": "results-linear.html#partial-dependence-plots",
    "title": "3  Linear Model",
    "section": "3.3 Partial Dependence Plots",
    "text": "3.3 Partial Dependence Plots\nWe now look at Partial Dependence Plots (PDPs) to investigate further the marginal influence of each feature throughout its whole range—holding all other variables constant. PDPs assist us to better understand how changes in individual feature values affect the estimated purchase probability of the model and are especially good for visualising possible nonlinear connections.\n\n\nCode\n# numeric variables (excluding target)\nnumeric_vars &lt;- sim_data %&gt;%\n  select(where(is.numeric)) %&gt;%\n  select(-purchase) %&gt;%\n  colnames()\n\n# Compute PDPs\npdp_list &lt;- lapply(numeric_vars, function(var) {\n  pd &lt;- pdp::partial(\n    object = log_model,\n    pred.var = var,\n    train = sim_data,\n    prob = TRUE\n  )\n  colnames(pd) &lt;- c(\"x\", \"yhat\")  # rename columns\n  pd$variable &lt;- var\n  return(pd)\n})\npdp_df &lt;- bind_rows(pdp_list)\n\n# Compute standard deviation of yhat per variable\nsd_order &lt;- pdp_df %&gt;%\n  group_by(variable) %&gt;%\n  summarise(sd_yhat = sd(yhat)) %&gt;%\n  arrange(desc(sd_yhat))\n\n# Reorder variable levels for facet_wrap\npdp_df$variable &lt;- factor(pdp_df$variable, levels = sd_order$variable)\n\n# Prepare rug data (long format)\nrug_df &lt;- sim_data %&gt;%\n  select(all_of(numeric_vars)) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"variable\", values_to = \"x\")\nrug_df$variable &lt;- factor(rug_df$variable, levels = sd_order$variable)\n\n# Final PDP plot with rug\nggplot(pdp_df, aes(x = x, y = yhat)) +\n  geom_line(color = \"blue\", linewidth = 0.9) +\n  geom_rug(data = rug_df, aes(x = x), \n           sides = \"b\", color = \"red\", alpha = 0.2, inherit.aes = FALSE) +\n  facet_wrap(~ variable, scales = \"free_x\") +\n  labs(title = \"PDP - Logistic\",\n       x = \"Feature Value\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThese plots offer valuable insights into model behavior and feature effects. Starting with category_count, we see a significant negative nonlinear association with purchase probability, those who browse several product categories are less likely to convert. Users lazily perusing several sorts of products may show decision weariness or a lack of buying intent. On the other hand, browsing_time reveals a consistent and obvious upward trend suggesting that people who spend more time on the website are more likely to buy. This probably indicates more involvement and thought. Likewise, items_in_cart is strongly positively correlated with purchase likelihood, hence supporting the idea that consumers with more items in their cart are nearer to complete the purchase. clicked_ad shows a positive increase in likelihood, implying that ad-clicking activity is a significant indicator of conversion intention. Finally, past_purchases reveals a little upward slope, which consistent with usual buyer retention trends, returning consumers are somewhat more inclined to buy again.\n\n3.3.1 LIME\nHowever, although these worldwide techniques help to grasp typical impacts throughout the whole dataset, they cannot clarify why a certain case got its distinctive forecast.\nBased on the results from the model and the two types of plots we have already generated, we have gained insight into the overall influence and trends of each individual feature on the predicted purchase probability. However, although we can gain the insights through these two plots we have already used, they fail to explain why a specific instance received its particular prediction. To address this limitation, we apply LIME, which provides localized explanations for individual predictions by approximating the model’s behavior near a given instance using a simple, interpretable model. By doing so, LIME allows us to uncover which specific features drove the prediction for each user, and in what direction. Here we choose to analysis the first three observation as our instance and perform the LIME plot on them.\n\n\nCode\nset.seed(5293)\nsim_data$purchase &lt;- as.factor(sim_data$purchase)\n\nmodel_type.glm &lt;- function(x, ...) 'classification'\npredict_model.glm &lt;- function(x, newdata, type, ...) {\n  preds &lt;- predict(x, newdata = newdata, type = \"response\")\n  data.frame(`1` = preds, `0` = 1 - preds)\n}\n\n# test train split(choose the first three observations as our instances) \ntest_data &lt;- sim_data[1:3, ]          \ntrain_data &lt;- sim_data[-(1:3), ]      \n\n# Fit logistic regression on training data\nlog_model &lt;- glm(purchase ~ ., data = train_data, family = \"binomial\")\n\n# explainer\nexplainer &lt;- lime::lime(\n  x = train_data %&gt;% select(-purchase),\n  model = log_model,\n  bin_continuous = TRUE\n)\n\n# Perform LIME on the test instances\nexplanation &lt;- lime::explain(\n  x = test_data %&gt;% select(-purchase),\n  explainer = explainer,\n  n_labels = 1,\n  n_features = 3\n)\nprint(explanation)\n\n\n# A tibble: 9 × 13\n  model_type    case  label label_prob model_r2 model_intercept model_prediction\n  &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;\n1 classificati… 1     X0         0.670    0.132           0.377            0.818\n2 classificati… 1     X0         0.670    0.132           0.377            0.818\n3 classificati… 1     X0         0.670    0.132           0.377            0.818\n4 classificati… 2     X1         0.743    0.520           0.327            0.499\n5 classificati… 2     X1         0.743    0.520           0.327            0.499\n6 classificati… 2     X1         0.743    0.520           0.327            0.499\n7 classificati… 3     X1         0.902    0.494           0.386            0.557\n8 classificati… 3     X1         0.902    0.494           0.386            0.557\n9 classificati… 3     X1         0.902    0.494           0.386            0.557\n# ℹ 6 more variables: feature &lt;chr&gt;, feature_value &lt;dbl&gt;, feature_weight &lt;dbl&gt;,\n#   feature_desc &lt;chr&gt;, data &lt;list&gt;, prediction &lt;list&gt;\n\n\nCode\nplot_features(explanation)\n\n\n\n\n\n\n\n\n\n\nFor case 1: The model correctly predicted no purchase, supported by short browsing time, moderate category count, and an low number of product or empty cart. However, the low Explanation Fit (0.13) means the local linear approximation by LIME does not well represent the model’s decision function in this region.\nFor case 2: Although the user had a short browsing time, the moderate number of items in cart and narrow category exploration were enough to strongly suggest a purchase, which is different from the ground truth. This might means that two strong positive features outweighed one weak negative, but perhaps over-optimistically.\nFor case 3: This prediction is correct that despite low cart items and short browsing time, the narrow category range played a dominant supporting role, leading to a correct purchase prediction.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Model</span>"
    ]
  },
  {
    "objectID": "results-non-linear.html",
    "href": "results-non-linear.html",
    "title": "4  Non Linear Model",
    "section": "",
    "text": "4.1 Fit the Random Forest Model\nWe trained the model on the same dataset as the linear models, sim_data, and fit the model by buliding 100 decision trees with randomly choose two variables at every split. To assist further interpretation, the model also monitors variable relevance.\nCode\n# Fit the random forest model\nrf_model &lt;- randomForest(\n  purchase ~ .,\n  data = sim_data,\n  ntree = 100,\n  importance = TRUE\n)\n\n# View model summary\nprint(rf_model)\n\n\n\nCall:\n randomForest(formula = purchase ~ ., data = sim_data, ntree = 100,      importance = TRUE) \n               Type of random forest: regression\n                     Number of trees: 100\nNo. of variables tried at each split: 2\n\n          Mean of squared residuals: 0.1628973\n                    % Var explained: 33.54\nBased on the summary result of the model we trained, the Out-of-Bag (OOB) error estimate is 25%, indicating that about 75% of observations were accurately predicted on average by the ensemble. The confusion matrix also shows class-specific errors: a 32% misclassification rate for class no purchase and a 20% misclassification rate for purchase.\nFor the AVPs, while are useful tools for linear models, they are not suitable for non-linear models like Random Forests. This is because AVPs are based on the assumption of linear additivity that they visualize the marginal effect of one predictor on the response after adjusting for others in a linear regression setting. Hence, we rely on the other two plots, PDP and LIME, to interpret non-linear models more appropriately.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Non Linear Model</span>"
    ]
  },
  {
    "objectID": "results-non-linear.html#partial-dependence-plots",
    "href": "results-non-linear.html#partial-dependence-plots",
    "title": "4  Non Linear Model",
    "section": "4.2 Partial Dependence Plots",
    "text": "4.2 Partial Dependence Plots\nIn non-linear models, PDPs help us to better understand how each feature influences the projections of our non-linear model. Even when the model incorporates complex, non-linear interactions, this allows us to see the marginal influence of every variable.\n\n\nCode\n# Extract numeric variables from sim_data\nnum_vars &lt;- sim_data %&gt;%\n  select(where(is.numeric)) %&gt;%\n  colnames()\n\n# Prepare rug data for plotting\nrug_df &lt;- sim_data %&gt;%\n  select(all_of(num_vars)) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"variable\", values_to = \"x\")\n\n# Create PDPs for each numeric feature\npdp_list_rf &lt;- lapply(num_vars, function(var) {\n  pd &lt;- pdp::partial(\n    object = rf_model,\n    pred.var = var,\n    train = sim_data,\n    prob = TRUE\n  )\n  names(pd)[1] &lt;- \"x\"\n  pd$variable &lt;- var\n  pd\n})\n\n# Combine into single data frame\npdp_df_rf &lt;- bind_rows(pdp_list_rf)\n\n# Sort variables by standard deviation\nsd_order &lt;- pdp_df_rf %&gt;%\n  group_by(variable) %&gt;%\n  summarise(sd_yhat = sd(yhat)) %&gt;%\n  arrange(desc(sd_yhat))\n\n# Reorder for plotting\npdp_df_rf$variable &lt;- factor(as.character(pdp_df_rf$variable),\n                             levels = as.character(sd_order$variable))\nrug_df$variable &lt;- factor(as.character(rug_df$variable),\n                          levels = as.character(sd_order$variable))\n\n# Plot\nlibrary(ggplot2)\nggplot(pdp_df_rf, aes(x = x, y = yhat)) +\n  geom_line(color = \"blue\", linewidth = 0.7) +\n  geom_rug(data = rug_df, aes(x = x), color = \"red\", alpha = 0.2, inherit.aes = FALSE) +\n  facet_wrap(~ variable, scales = \"free_x\") +\n  labs(title = \"PDP – Random Forest\",\n       x = \"Feature Value\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nBased on the result from PDP plots, which we plot then in descending standard deviation order, we analysis their trends case by case.\ncategory_count: it shows a stong upward trend, suggesting that consumers who explore more product categories are considerably more likely to buy, probably because of more involvement or interest. browsing_time reveals an inverted U-shaped correlation, which means the probability of purchasing rises first but falls significantly after 2.5–3 hours, implying that prolonged browsing could indicate not purchase rather than buying. clicked_ad shows a negative linear trend as well, suggesting that people who click on advertising are really less likely to purchase. Likewise, items_in_cart has a significant negative slope that could suggest more items in the cart correspond to lower purchase probability. Lastly, previous_purchases presents a slight negative effect, suggesting that users with previous purchases show a slightly lower probability of purchasing again.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Non Linear Model</span>"
    ]
  },
  {
    "objectID": "comparsion.html",
    "href": "comparsion.html",
    "title": "5  Compasrion between two models",
    "section": "",
    "text": "5.1 PDP",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Compasrion between two models</span>"
    ]
  },
  {
    "objectID": "comparsion.html#lime",
    "href": "comparsion.html#lime",
    "title": "5  Compasrion between two models",
    "section": "5.2 LIME",
    "text": "5.2 LIME",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Compasrion between two models</span>"
    ]
  },
  {
    "objectID": "results-linear.html#fit-the-random-forest-model",
    "href": "results-linear.html#fit-the-random-forest-model",
    "title": "3  Linear Model",
    "section": "4.1 Fit the Random Forest Model",
    "text": "4.1 Fit the Random Forest Model\nWe trained the model on the same dataset as the linear models, sim_data, and fit the model by buliding 100 decision trees with randomly choose two variables at every split. To assist further interpretation, the model also monitors variable relevance.\n\n\nCode\nlibrary(randomForest)\n# Fit the random forest model\nrf_model &lt;- randomForest(\n  purchase ~ .,\n  data = sim_data,\n  ntree = 100,\n  importance = TRUE\n)\n\n# View model summary\nprint(rf_model)\n\n\n\nCall:\n randomForest(formula = purchase ~ ., data = sim_data, ntree = 100,      importance = TRUE) \n               Type of random forest: classification\n                     Number of trees: 100\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 25%\nConfusion matrix:\n    0   1 class.error\n0 146  69   0.3209302\n1  56 229   0.1964912\n\n\nBased on the summary result of the model we trained, the Out-of-Bag (OOB) error estimate is 25%, indicating that about 75% of observations were accurately predicted on average by the ensemble. The confusion matrix also shows class-specific errors: a 32% misclassification rate for class no purchase and a 20% misclassification rate for purchase.\nFor the AVPs, while are useful tools for linear models, they are not suitable for non-linear models like Random Forests. This is because AVPs are based on the assumption of linear additivity that they visualize the marginal effect of one predictor on the response after adjusting for others in a linear regression setting. Hence, we rely on the other two plots, PDP and LIME, to interpret non-linear models more appropriately.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Model</span>"
    ]
  },
  {
    "objectID": "results-linear.html#partial-dependence-plots-1",
    "href": "results-linear.html#partial-dependence-plots-1",
    "title": "3  Linear Model",
    "section": "4.2 Partial Dependence Plots",
    "text": "4.2 Partial Dependence Plots\nIn non-linear models, PDPs help us to better understand how each feature influences the projections of our non-linear model. Even when the model incorporates complex, non-linear interactions, this allows us to see the marginal influence of every variable.\n\n\nCode\n# Extract numeric variables from sim_data\nnum_vars &lt;- sim_data %&gt;%\n  select(where(is.numeric)) %&gt;%\n  colnames()\n\n# Prepare rug data for plotting\nrug_df &lt;- sim_data %&gt;%\n  select(all_of(num_vars)) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"variable\", values_to = \"x\")\n\n# Create PDPs for each numeric feature\npdp_list_rf &lt;- lapply(num_vars, function(var) {\n  pd &lt;- pdp::partial(\n    object = rf_model,\n    pred.var = var,\n    train = sim_data,\n    prob = TRUE\n  )\n  names(pd)[1] &lt;- \"x\"\n  pd$variable &lt;- var\n  pd\n})\n\n# Combine into single data frame\npdp_df_rf &lt;- bind_rows(pdp_list_rf)\n\n# Sort variables by standard deviation\nsd_order &lt;- pdp_df_rf %&gt;%\n  group_by(variable) %&gt;%\n  summarise(sd_yhat = sd(yhat)) %&gt;%\n  arrange(desc(sd_yhat))\n\n# Reorder for plotting\npdp_df_rf$variable &lt;- factor(as.character(pdp_df_rf$variable),\n                             levels = as.character(sd_order$variable))\nrug_df$variable &lt;- factor(as.character(rug_df$variable),\n                          levels = as.character(sd_order$variable))\n\n# Plot\nlibrary(ggplot2)\nggplot(pdp_df_rf, aes(x = x, y = yhat)) +\n  geom_line(color = \"blue\", linewidth = 0.7) +\n  geom_rug(data = rug_df, aes(x = x), color = \"red\", alpha = 0.2, inherit.aes = FALSE) +\n  facet_wrap(~ variable, scales = \"free_x\") +\n  labs(title = \"PDP – Random Forest\",\n       x = \"Feature Value\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nBased on the result from PDP plots, which we plot then in descending standard deviation order, we analysis their trends case by case.\ncategory_count: it shows a stong upward trend, suggesting that consumers who explore more product categories are considerably more likely to buy, probably because of more involvement or interest. browsing_time reveals an inverted U-shaped correlation, which means the probability of purchasing rises first but falls significantly after 2.5–3 hours, implying that prolonged browsing could indicate not purchase rather than buying. clicked_ad shows a negative linear trend as well, suggesting that people who click on advertising are really less likely to purchase. Likewise, items_in_cart has a significant negative slope that could suggest more items in the cart correspond to lower purchase probability. Lastly, previous_purchases presents a slight negative effect, suggesting that users with previous purchases show a slightly lower probability of purchasing again.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Model</span>"
    ]
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "3  Results of Linear and Non-linear Models",
    "section": "",
    "text": "3.1 Linear Model\nWe now move to construct a predictive model after simulating our dataset with important behavioral characteristics connected to online buying. Ou response variable buy being binary (1 = purchased, 0 = no purchase), we estimate the probability of purchase as a function of the six explanatory variables using a Logistic Regression model. It is a suitable option that models the log-odds of purchase as a linear mixture of the six characteristics, given that the outcome variable is binary. The coefficient predicted by the logistic model here could vary from the “ground truth” coefficients we used to simulated the dataset. Added random noise and sample variation cause this difference, which more genuinely represents uncertainty in a real-world data. We estimate how well the model recovers the underlying data-generating process by contrasting the projected coefficients with the known ground truth.\nBefore the modelling process, install all packages:\nCode\n# packages\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(pdp)\nlibrary(lime)\nlibrary(randomForest)\nlibrary(vip)\nAnd load the dataset we simulated before:\nCode\nset.seed(5293)\nn &lt;- 500\n\n# Generate the features for the dataset\nbrowsing_time &lt;- rnorm(n, mean = 3, sd = 1)               # hours\ncategory_count &lt;- rpois(n, lambda = 4)                    # integer count\nclicked_ad &lt;- rbinom(n, 1, prob = 0.35)                   # binary\nitems_in_cart &lt;- rpois(n, lambda = 2)                     # integer count\ndevice_type &lt;- rbinom(n, 1, prob = 0.6)                   # 0 = desktop, 1 = mobile\nprevious_purchases &lt;- rbinom(n, 1, prob = 0.25)           # binary\nnoise &lt;- rnorm(n, 0, 0.5)\n\n# Simulate outcome\nlogit &lt;- -1 + \n  0.8 * browsing_time - \n  0.6 * category_count + \n  1.2 * clicked_ad + \n  0.5 * items_in_cart + \n  0.4 * previous_purchases +\n  0.3 * device_type +\n  noise\n\np &lt;- 1 / (1 + exp(-logit))\npurchase &lt;- rbinom(n, 1, p)\n\n# Assemble data frame\nsim_data &lt;- data.frame(\n  browsing_time,\n  category_count,\n  clicked_ad,\n  items_in_cart,\n  device_type = factor(device_type, levels = c(0, 1), labels = c(\"desktop\", \"mobile\")),\n  previous_purchases,\n  purchase\n)\n#head(sim_data)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results of Linear and Non-linear Models</span>"
    ]
  },
  {
    "objectID": "results.html#added-variable-plots",
    "href": "results.html#added-variable-plots",
    "title": "3  Linear Model",
    "section": "3.2 Added Variable Plots",
    "text": "3.2 Added Variable Plots\nTo further analysis the contribution of each individual predictors, we use the Added Variable Plot (AVP) to more fully considering the influences of other variables. By graphing the response residuals (after regressing out the other predictors) against the predictor residuals (after regressing it out from the other predictors as well), this diagnostic tool shows the partial link between the outcome and a chosen predictor.\n\n\nCode\nlibrary(tidyverse)\nlibrary(broom)\n\n# numeric predictors\nnumeric_vars &lt;- sim_data %&gt;%\n  select(where(is.numeric)) %&gt;%\n  select(-purchase) %&gt;%\n  colnames()\n\n# compute AVP residuals and SD of fit per feature\navp_list &lt;- lapply(numeric_vars, function(var) {\n  other_vars &lt;- setdiff(numeric_vars, var)\n  model_y &lt;- glm(as.formula(paste(\"purchase ~\", paste(other_vars, collapse = \" + \"))),\n                 data = sim_data, family = \"binomial\")\n  res_y &lt;- resid(model_y)\n  model_x &lt;- lm(as.formula(paste(var, \"~\", paste(other_vars, collapse = \" + \"))),\n                data = sim_data)\n  res_x &lt;- resid(model_x)\n  avp_fit &lt;- lm(res_y ~ res_x)\n  fit_vals &lt;- predict(avp_fit)\n  sd_fit &lt;- sd(fit_vals)\n  tibble(\n    feature = var,\n    res_x = res_x,\n    res_y = res_y,\n    fit = fit_vals,\n    sd_fit = sd_fit\n  )\n})\n\n# Combine into dataframe\navp_df &lt;- bind_rows(avp_list)\n\n# Order by descending SD\nsd_order &lt;- avp_df %&gt;%\n  group_by(feature) %&gt;%\n  summarise(sd_fit = first(sd_fit)) %&gt;%\n  arrange(desc(sd_fit))\n\navp_df$feature &lt;- factor(avp_df$feature, levels = sd_order$feature)\n\n# Plot \nggplot(avp_df, aes(x = res_x, y = res_y)) +\n  geom_point(alpha = 0.4) +\n  geom_smooth(method = \"lm\", color = \"blue\", linewidth = 0.8) +\n  facet_wrap(~ feature, scales = \"free_x\") +\n  labs(\n    title = \"Added Variable Plots\",\n    x = \"Feature residual (unexplained by other predictors)\",\n    y = \"Purchase residual (unexplained by other predictors)\"\n  ) +\n  theme_minimal()\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAfter considering all additional factors in the logistic regression model, the AVPs suggests the marginal influence of each predictor on the binary response variable purchase. Every panel displays the residual of a specified characteristic (also adjusted for other variables) plotted against the residual of purchase (after eliminating the influence of other predictors). A prominent linear trend in the graphic indicates that the related predictor significantly affects the model.\nThe graphs reveal that category_count has a substantial negative linear correlation with the response residuals, suggesting a notable negative link with the likelihood of purchase even after other attributes are controlled. All three variables, browsing_time, clicked_ad, and items_in_cart, show clear positive linear trends consistent with the idea that more browsing, clicks, and cart activity usually raise the probability of purchase. The plot for previous_purchases shows a weaker and less clear trend, suggesting its marginal contribution is smaller, which is consistent with its less significant coefficient in the logistic regression we got above at fitting the model. At last, device_type would fit its high p-value in the regression summary and may not be as important. These graphs show generally where the linear model fits well and help visually verify the significance of the strongest predictors.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Model</span>"
    ]
  },
  {
    "objectID": "results.html#partial-dependence-plots",
    "href": "results.html#partial-dependence-plots",
    "title": "3  Linear Model",
    "section": "3.3 Partial Dependence Plots",
    "text": "3.3 Partial Dependence Plots\nWe now look at Partial Dependence Plots (PDPs) to investigate further the marginal influence of each feature throughout its whole range—holding all other variables constant. PDPs assist us to better understand how changes in individual feature values affect the estimated purchase probability of the model and are especially good for visualising possible nonlinear connections.\n\n\nCode\n# numeric variables (excluding target)\nnumeric_vars &lt;- sim_data %&gt;%\n  select(where(is.numeric)) %&gt;%\n  select(-purchase) %&gt;%\n  colnames()\n\n# Compute PDPs\npdp_list &lt;- lapply(numeric_vars, function(var) {\n  pd &lt;- pdp::partial(\n    object = log_model,\n    pred.var = var,\n    train = sim_data,\n    prob = TRUE\n  )\n  colnames(pd) &lt;- c(\"x\", \"yhat\")  # rename columns\n  pd$variable &lt;- var\n  return(pd)\n})\npdp_df &lt;- bind_rows(pdp_list)\n\n# Compute standard deviation of yhat per variable\nsd_order &lt;- pdp_df %&gt;%\n  group_by(variable) %&gt;%\n  summarise(sd_yhat = sd(yhat)) %&gt;%\n  arrange(desc(sd_yhat))\n\n# Reorder variable levels for facet_wrap\npdp_df$variable &lt;- factor(pdp_df$variable, levels = sd_order$variable)\n\n# Prepare rug data (long format)\nrug_df &lt;- sim_data %&gt;%\n  select(all_of(numeric_vars)) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"variable\", values_to = \"x\")\nrug_df$variable &lt;- factor(rug_df$variable, levels = sd_order$variable)\n\n# Final PDP plot with rug\nggplot(pdp_df, aes(x = x, y = yhat)) +\n  geom_line(color = \"blue\", linewidth = 0.9) +\n  geom_rug(data = rug_df, aes(x = x), \n           sides = \"b\", color = \"red\", alpha = 0.2, inherit.aes = FALSE) +\n  facet_wrap(~ variable, scales = \"free_x\") +\n  labs(title = \"PDP - Logistic\",\n       x = \"Feature Value\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThese plots offer valuable insights into model behavior and feature effects. Starting with category_count, we see a significant negative nonlinear association with purchase probability, those who browse several product categories are less likely to convert. Users lazily perusing several sorts of products may show decision weariness or a lack of buying intent. On the other hand, browsing_time reveals a consistent and obvious upward trend suggesting that people who spend more time on the website are more likely to buy. This probably indicates more involvement and thought. Likewise, items_in_cart is strongly positively correlated with purchase likelihood, hence supporting the idea that consumers with more items in their cart are nearer to complete the purchase. clicked_ad shows a positive increase in likelihood, implying that ad-clicking activity is a significant indicator of conversion intention. Finally, past_purchases reveals a little upward slope, which consistent with usual buyer retention trends, returning consumers are somewhat more inclined to buy again.\n\n3.3.1 LIME\nHowever, although these worldwide techniques help to grasp typical impacts throughout the whole dataset, they cannot clarify why a certain case got its distinctive forecast.\nBased on the results from the model and the two types of plots we have already generated, we have gained insight into the overall influence and trends of each individual feature on the predicted purchase probability. However, although we can gain the insights through these two plots we have already used, they fail to explain why a specific instance received its particular prediction. To address this limitation, we apply LIME, which provides localized explanations for individual predictions by approximating the model’s behavior near a given instance using a simple, interpretable model. By doing so, LIME allows us to uncover which specific features drove the prediction for each user, and in what direction. Here we choose to analysis the first three observation as our instance and perform the LIME plot on them.\n\n\nCode\nset.seed(5293)\nsim_data$purchase &lt;- as.factor(sim_data$purchase)\n\nmodel_type.glm &lt;- function(x, ...) 'classification'\npredict_model.glm &lt;- function(x, newdata, type, ...) {\n  preds &lt;- predict(x, newdata = newdata, type = \"response\")\n  data.frame(`1` = preds, `0` = 1 - preds)\n}\n\n# test train split(choose the first three observations as our instances) \ntest_data &lt;- sim_data[1:3, ]          \ntrain_data &lt;- sim_data[-(1:3), ]      \n\n# Fit logistic regression on training data\nlog_model &lt;- glm(purchase ~ ., data = train_data, family = \"binomial\")\n\n# explainer\nexplainer &lt;- lime::lime(\n  x = train_data %&gt;% select(-purchase),\n  model = log_model,\n  bin_continuous = TRUE\n)\n\n# Perform LIME on the test instances\nexplanation &lt;- lime::explain(\n  x = test_data %&gt;% select(-purchase),\n  explainer = explainer,\n  n_labels = 1,\n  n_features = 3\n)\nprint(explanation)\n\n\n# A tibble: 9 × 13\n  model_type    case  label label_prob model_r2 model_intercept model_prediction\n  &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;\n1 classificati… 1     X0         0.670    0.132           0.377            0.818\n2 classificati… 1     X0         0.670    0.132           0.377            0.818\n3 classificati… 1     X0         0.670    0.132           0.377            0.818\n4 classificati… 2     X1         0.743    0.520           0.327            0.499\n5 classificati… 2     X1         0.743    0.520           0.327            0.499\n6 classificati… 2     X1         0.743    0.520           0.327            0.499\n7 classificati… 3     X1         0.902    0.494           0.386            0.557\n8 classificati… 3     X1         0.902    0.494           0.386            0.557\n9 classificati… 3     X1         0.902    0.494           0.386            0.557\n# ℹ 6 more variables: feature &lt;chr&gt;, feature_value &lt;dbl&gt;, feature_weight &lt;dbl&gt;,\n#   feature_desc &lt;chr&gt;, data &lt;list&gt;, prediction &lt;list&gt;\n\n\nCode\nplot_features(explanation)\n\n\n\n\n\n\n\n\n\n\nFor case 1: The model correctly predicted no purchase, supported by short browsing time, moderate category count, and an low number of product or empty cart. However, the low Explanation Fit (0.13) means the local linear approximation by LIME does not well represent the model’s decision function in this region.\nFor case 2: Although the user had a short browsing time, the moderate number of items in cart and narrow category exploration were enough to strongly suggest a purchase, which is different from the ground truth. This might means that two strong positive features outweighed one weak negative, but perhaps over-optimistically.\nFor case 3: This prediction is correct that despite low cart items and short browsing time, the narrow category range played a dominant supporting role, leading to a correct purchase prediction.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Model</span>"
    ]
  },
  {
    "objectID": "results.html#fit-the-random-forest-model",
    "href": "results.html#fit-the-random-forest-model",
    "title": "3  Linear Model",
    "section": "4.1 Fit the Random Forest Model",
    "text": "4.1 Fit the Random Forest Model\nWe trained the model on the same dataset as the linear models, sim_data, and fit the model by buliding 100 decision trees with randomly choose two variables at every split. To assist further interpretation, the model also monitors variable relevance.\n\n\nCode\nlibrary(randomForest)\n# Fit the random forest model\nrf_model &lt;- randomForest(\n  purchase ~ .,\n  data = sim_data,\n  ntree = 100,\n  importance = TRUE\n)\n\n# View model summary\nprint(rf_model)\n\n\n\nCall:\n randomForest(formula = purchase ~ ., data = sim_data, ntree = 100,      importance = TRUE) \n               Type of random forest: classification\n                     Number of trees: 100\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 25%\nConfusion matrix:\n    0   1 class.error\n0 146  69   0.3209302\n1  56 229   0.1964912\n\n\nBased on the summary result of the model we trained, the Out-of-Bag (OOB) error estimate is 25%, indicating that about 75% of observations were accurately predicted on average by the ensemble. The confusion matrix also shows class-specific errors: a 32% misclassification rate for class no purchase and a 20% misclassification rate for purchase.\nFor the AVPs, while are useful tools for linear models, they are not suitable for non-linear models like Random Forests. This is because AVPs are based on the assumption of linear additivity that they visualize the marginal effect of one predictor on the response after adjusting for others in a linear regression setting. Hence, we rely on the other two plots, PDP and LIME, to interpret non-linear models more appropriately.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Model</span>"
    ]
  },
  {
    "objectID": "results.html#partial-dependence-plots-1",
    "href": "results.html#partial-dependence-plots-1",
    "title": "3  Linear Model",
    "section": "4.2 Partial Dependence Plots",
    "text": "4.2 Partial Dependence Plots\nIn non-linear models, PDPs help us to better understand how each feature influences the projections of our non-linear model. Even when the model incorporates complex, non-linear interactions, this allows us to see the marginal influence of every variable.\n\n\nCode\n# Extract numeric variables from sim_data\nnum_vars &lt;- sim_data %&gt;%\n  select(where(is.numeric)) %&gt;%\n  colnames()\n\n# Prepare rug data for plotting\nrug_df &lt;- sim_data %&gt;%\n  select(all_of(num_vars)) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"variable\", values_to = \"x\")\n\n# Create PDPs for each numeric feature\npdp_list_rf &lt;- lapply(num_vars, function(var) {\n  pd &lt;- pdp::partial(\n    object = rf_model,\n    pred.var = var,\n    train = sim_data,\n    prob = TRUE\n  )\n  names(pd)[1] &lt;- \"x\"\n  pd$variable &lt;- var\n  pd\n})\n\n# Combine into single data frame\npdp_df_rf &lt;- bind_rows(pdp_list_rf)\n\n# Sort variables by standard deviation\nsd_order &lt;- pdp_df_rf %&gt;%\n  group_by(variable) %&gt;%\n  summarise(sd_yhat = sd(yhat)) %&gt;%\n  arrange(desc(sd_yhat))\n\n# Reorder for plotting\npdp_df_rf$variable &lt;- factor(as.character(pdp_df_rf$variable),\n                             levels = as.character(sd_order$variable))\nrug_df$variable &lt;- factor(as.character(rug_df$variable),\n                          levels = as.character(sd_order$variable))\n\n# Plot\nlibrary(ggplot2)\nggplot(pdp_df_rf, aes(x = x, y = yhat)) +\n  geom_line(color = \"blue\", linewidth = 0.7) +\n  geom_rug(data = rug_df, aes(x = x), color = \"red\", alpha = 0.2, inherit.aes = FALSE) +\n  facet_wrap(~ variable, scales = \"free_x\") +\n  labs(title = \"PDP – Random Forest\",\n       x = \"Feature Value\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nBased on the result from PDP plots, which we plot then in descending standard deviation order, we analysis their trends case by case.\ncategory_count: it shows a stong upward trend, suggesting that consumers who explore more product categories are considerably more likely to buy, probably because of more involvement or interest. browsing_time reveals an inverted U-shaped correlation, which means the probability of purchasing rises first but falls significantly after 2.5–3 hours, implying that prolonged browsing could indicate not purchase rather than buying. clicked_ad shows a negative linear trend as well, suggesting that people who click on advertising are really less likely to purchase. Likewise, items_in_cart has a significant negative slope that could suggest more items in the cart correspond to lower purchase probability. Lastly, previous_purchases presents a slight negative effect, suggesting that users with previous purchases show a slightly lower probability of purchasing again.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Model</span>"
    ]
  },
  {
    "objectID": "results.html#linear-model",
    "href": "results.html#linear-model",
    "title": "3  Results of Linear and Non-linear Models",
    "section": "",
    "text": "3.1.1 Fit the Linear Model (logistic regression)\n\n\nCode\nsim_data &lt;- sim_data\n# fit the logistic regression model\nlog_model &lt;- glm(purchase ~ ., data = sim_data, family = \"binomial\")\n\nsummary(log_model)\n\n\n\nCall:\nglm(formula = purchase ~ ., family = \"binomial\", data = sim_data)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        -1.11467    0.51565  -2.162  0.03064 *  \nbrowsing_time       0.84165    0.12942   6.503 7.85e-11 ***\ncategory_count     -0.72339    0.07769  -9.311  &lt; 2e-16 ***\nclicked_ad          1.61928    0.26507   6.109 1.00e-09 ***\nitems_in_cart       0.58558    0.09632   6.079 1.21e-09 ***\ndevice_typemobile   0.07205    0.24024   0.300  0.76424    \nprevious_purchases  0.76285    0.28375   2.688  0.00718 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 683.31  on 499  degrees of freedom\nResidual deviance: 438.31  on 493  degrees of freedom\nAIC: 452.31\n\nNumber of Fisher Scoring iterations: 5\n\n\nBased on the logistic model we generated, the summary result shows that statistically significant predictors include browsing_time, category_count, clicked_ad, items_in_cart, and previous_purchases. This means that these five feature have important impact in purchase behavior. However, device_type is not significant (p = 0.764), indicating that switching from desktop to mobile does not have a strong independent effect on the likelihood of purchase in this model. Signs of coefficients match the logic of the simulation (positive for most predictors, negative for category count), though values differ slightly due to added noise. Additionally, the residual deviance of 438.31, compared to the null deviance of 683.31, suggests that the model explains a substantial portion of the variability in the outcome. By adding these six predictors, the deviance reduced from 683.31 to 438.31, means that our features signigicantly improve the model’s predicting ability. The overall logistic model at this step can be written as:\n\\[\\begin{align}\n\\log\\left(\\frac{P(\\text{purchase} = 1)}{1 - P(\\text{purchase} = 1)}\\right) &=\n-1.1147 + 0.8417 \\cdot \\text{browsing\\_time} \\\\\n&\\quad - 0.7234 \\cdot \\text{category\\_count}\n+ 1.6193 \\cdot \\text{clicked\\_ad} \\\\\n&\\quad + 0.5856 \\cdot \\text{items\\_in\\_cart}\n+ 0.0721 \\cdot \\text{device\\_type}_{\\text{mobile}} \\\\\n&\\quad + 0.7629 \\cdot \\text{previous\\_purchases}\n\\end{align}\\]\n\n\n3.1.2 Added Variable Plots\nTo further analysis the contribution of each individual predictors, we use the Added Variable Plot (AVP) to more fully considering the influences of other variables. By graphing the response residuals (after regressing out the other predictors) against the predictor residuals (after regressing it out from the other predictors as well), this diagnostic tool shows the partial link between the outcome and a chosen predictor.\n\n\nCode\nlibrary(tidyverse)\nlibrary(broom)\n\n# numeric predictors\nnumeric_vars &lt;- sim_data %&gt;%\n  select(where(is.numeric)) %&gt;%\n  select(-purchase) %&gt;%\n  colnames()\n\n# compute AVP residuals and SD of fit per feature\navp_list &lt;- lapply(numeric_vars, function(var) {\n  other_vars &lt;- setdiff(numeric_vars, var)\n  model_y &lt;- glm(as.formula(paste(\"purchase ~\", paste(other_vars, collapse = \" + \"))),\n                 data = sim_data, family = \"binomial\")\n  res_y &lt;- resid(model_y)\n  model_x &lt;- lm(as.formula(paste(var, \"~\", paste(other_vars, collapse = \" + \"))),\n                data = sim_data)\n  res_x &lt;- resid(model_x)\n  avp_fit &lt;- lm(res_y ~ res_x)\n  fit_vals &lt;- predict(avp_fit)\n  sd_fit &lt;- sd(fit_vals)\n  tibble(\n    feature = var,\n    res_x = res_x,\n    res_y = res_y,\n    fit = fit_vals,\n    sd_fit = sd_fit\n  )\n})\n\n# Combine into dataframe\navp_df &lt;- bind_rows(avp_list)\n\n# Order by descending SD\nsd_order &lt;- avp_df %&gt;%\n  group_by(feature) %&gt;%\n  summarise(sd_fit = first(sd_fit)) %&gt;%\n  arrange(desc(sd_fit))\n\navp_df$feature &lt;- factor(avp_df$feature, levels = sd_order$feature)\n\n# Plot \nggplot(avp_df, aes(x = res_x, y = res_y)) +\n  geom_point(alpha = 0.4) +\n  geom_smooth(method = \"lm\", color = \"blue\", linewidth = 0.8) +\n  facet_wrap(~ feature, scales = \"free_x\") +\n  labs(\n    title = \"Added Variable Plots\",\n    x = \"Feature residual (unexplained by other predictors)\",\n    y = \"Purchase residual (unexplained by other predictors)\"\n  ) +\n  theme_minimal()\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAfter considering all additional factors in the logistic regression model, the AVPs suggests the marginal influence of each predictor on the binary response variable purchase. Every panel displays the residual of a specified characteristic (also adjusted for other variables) plotted against the residual of purchase (after eliminating the influence of other predictors). A prominent linear trend in the graphic indicates that the related predictor significantly affects the model.\nThe graphs reveal that category_count has a substantial negative linear correlation with the response residuals, suggesting a notable negative link with the likelihood of purchase even after other attributes are controlled. All three variables, browsing_time, clicked_ad, and items_in_cart, show clear positive linear trends consistent with the idea that more browsing, clicks, and cart activity usually raise the probability of purchase. The plot for previous_purchases shows a weaker and less clear trend, suggesting its marginal contribution is smaller, which is consistent with its less significant coefficient in the logistic regression we got above at fitting the model. At last, device_type would fit its high p-value in the regression summary and may not be as important. These graphs show generally where the linear model fits well and help visually verify the significance of the strongest predictors.\n\n\n3.1.3 Partial Dependence Plots\nWe now look at Partial Dependence Plots (PDPs) to investigate further the marginal influence of each feature throughout its whole range—holding all other variables constant. PDPs assist us to better understand how changes in individual feature values affect the estimated purchase probability of the model and are especially good for visualising possible nonlinear connections.\n\n\nCode\n# numeric variables (excluding target)\nnumeric_vars &lt;- sim_data %&gt;%\n  select(where(is.numeric)) %&gt;%\n  select(-purchase) %&gt;%\n  colnames()\n\n# Compute PDPs\npdp_list &lt;- lapply(numeric_vars, function(var) {\n  pd &lt;- pdp::partial(\n    object = log_model,\n    pred.var = var,\n    train = sim_data,\n    prob = TRUE\n  )\n  colnames(pd) &lt;- c(\"x\", \"yhat\")  # rename columns\n  pd$variable &lt;- var\n  return(pd)\n})\npdp_df &lt;- bind_rows(pdp_list)\n\n# Compute standard deviation of yhat per variable\nsd_order &lt;- pdp_df %&gt;%\n  group_by(variable) %&gt;%\n  summarise(sd_yhat = sd(yhat)) %&gt;%\n  arrange(desc(sd_yhat))\n\n# Reorder variable levels for facet_wrap\npdp_df$variable &lt;- factor(pdp_df$variable, levels = sd_order$variable)\n\n# Prepare rug data (long format)\nrug_df &lt;- sim_data %&gt;%\n  select(all_of(numeric_vars)) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"variable\", values_to = \"x\")\nrug_df$variable &lt;- factor(rug_df$variable, levels = sd_order$variable)\n\n# Final PDP plot with rug\nggplot(pdp_df, aes(x = x, y = yhat)) +\n  geom_line(color = \"blue\", linewidth = 0.9) +\n  geom_rug(data = rug_df, aes(x = x), \n           sides = \"b\", color = \"red\", alpha = 0.2, inherit.aes = FALSE) +\n  facet_wrap(~ variable, scales = \"free_x\") +\n  labs(title = \"PDP - Logistic\",\n       x = \"Feature Value\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThese plots offer valuable insights into model behavior and feature effects. Starting with category_count, we see a significant negative nonlinear association with purchase probability, those who browse several product categories are less likely to convert. Users lazily perusing several sorts of products may show decision weariness or a lack of buying intent. On the other hand, browsing_time reveals a consistent and obvious upward trend suggesting that people who spend more time on the website are more likely to buy. This probably indicates more involvement and thought. Likewise, items_in_cart is strongly positively correlated with purchase likelihood, hence supporting the idea that consumers with more items in their cart are nearer to complete the purchase. clicked_ad shows a positive increase in likelihood, implying that ad-clicking activity is a significant indicator of conversion intention. Finally, past_purchases reveals a little upward slope, which consistent with usual buyer retention trends, returning consumers are somewhat more inclined to buy again.\n\n\n3.1.4 LIME\nBased on the results from the model and the two types of plots we have already generated, we have gained insight into the overall influence and trends of each individual feature on the predicted purchase probability. However, although we can gain the insights through these two plots we have already used, they fail to explain why a specific instance received its particular prediction. To address this limitation, we apply LIME, which provides localized explanations for individual predictions by approximating the model’s behavior near a given instance using a simple, interpretable model. By doing so, LIME allows us to uncover which specific features drove the prediction for each user, and in what direction. Here we choose to analysis the first three observation as our instance and perform the LIME plot on them.\n\n\nCode\nset.seed(5293)\nsim_data$purchase &lt;- as.factor(sim_data$purchase)\n\nmodel_type.glm &lt;- function(x, ...) 'classification'\npredict_model.glm &lt;- function(x, newdata, type, ...) {\n  preds &lt;- predict(x, newdata = newdata, type = \"response\")\n  data.frame(`1` = preds, `0` = 1 - preds)\n}\n\n# test train split(choose the first three observations as our instances) \ntest_data &lt;- sim_data[1:3, ]          \ntrain_data &lt;- sim_data[-(1:3), ]      \n\n# Fit logistic regression on training data\nlog_model &lt;- glm(purchase ~ ., data = train_data, family = \"binomial\")\n\n# explainer\nexplainer &lt;- lime::lime(\n  x = train_data %&gt;% select(-purchase),\n  model = log_model,\n  bin_continuous = TRUE\n)\n\n# Perform LIME on the test instances\nexplanation &lt;- lime::explain(\n  x = test_data %&gt;% select(-purchase),\n  explainer = explainer,\n  n_labels = 1,\n  n_features = 3\n)\nprint(explanation)\n\n\n# A tibble: 9 × 13\n  model_type    case  label label_prob model_r2 model_intercept model_prediction\n  &lt;chr&gt;         &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;\n1 classificati… 1     X0         0.670    0.132           0.377            0.818\n2 classificati… 1     X0         0.670    0.132           0.377            0.818\n3 classificati… 1     X0         0.670    0.132           0.377            0.818\n4 classificati… 2     X1         0.743    0.520           0.327            0.499\n5 classificati… 2     X1         0.743    0.520           0.327            0.499\n6 classificati… 2     X1         0.743    0.520           0.327            0.499\n7 classificati… 3     X1         0.902    0.494           0.386            0.557\n8 classificati… 3     X1         0.902    0.494           0.386            0.557\n9 classificati… 3     X1         0.902    0.494           0.386            0.557\n# ℹ 6 more variables: feature &lt;chr&gt;, feature_value &lt;dbl&gt;, feature_weight &lt;dbl&gt;,\n#   feature_desc &lt;chr&gt;, data &lt;list&gt;, prediction &lt;list&gt;\n\n\nCode\nplot_features(explanation)\n\n\n\n\n\n\n\n\n\n\nFor case 1: The model correctly predicted no purchase, supported by short browsing time, moderate category count, and an low number of product or empty cart. However, the low Explanation Fit (0.13) means the local linear approximation by LIME does not well represent the model’s decision function in this region.\nFor case 2: Although the user had a short browsing time, the moderate number of items in cart and narrow category exploration were enough to strongly suggest a purchase, which is different from the ground truth. This might means that two strong positive features outweighed one weak negative, but perhaps over-optimistically.\nFor case 3: This prediction is correct that despite low cart items and short browsing time, the narrow category range played a dominant supporting role, leading to a correct purchase prediction.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results of Linear and Non-linear Models</span>"
    ]
  },
  {
    "objectID": "results.html#non-linear-model",
    "href": "results.html#non-linear-model",
    "title": "3  Results of Linear and Non-linear Models",
    "section": "3.2 Non-Linear Model",
    "text": "3.2 Non-Linear Model\nBased on the result we got previously from the linear model, we now aim to further investigate some more flexible model. Although the linear model gave us a strong basis and let us understand the patterns and individual feature contributions, it is naturally constrained in its capacity to capture non-linear interactions between variables. Hence we trained a Random Forest mode, which build many decision trees and combining their results, to helps us to explore the interaction effects and they can capture complex, non-linear relationships and interactions between variables. Furthermore, Random Forest is still compatible with interpretation tools like as Permutation Importance, PDP, and LIME, hence allowing a consistent and understandable analysis across modelling techniques.\n\n3.2.1 Fit the Random Forest Model\nWe trained the model on the same dataset as the linear models, sim_data, and fit the model by buliding 100 decision trees with randomly choose two variables at every split. To assist further interpretation, the model also monitors variable relevance.\n\n\nCode\nlibrary(randomForest)\n# Fit the random forest model\nrf_model &lt;- randomForest(\n  purchase ~ .,\n  data = sim_data,\n  ntree = 100,\n  importance = TRUE\n)\n\n# View model summary\nprint(rf_model)\n\n\n\nCall:\n randomForest(formula = purchase ~ ., data = sim_data, ntree = 100,      importance = TRUE) \n               Type of random forest: classification\n                     Number of trees: 100\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 25%\nConfusion matrix:\n    0   1 class.error\n0 146  69   0.3209302\n1  56 229   0.1964912\n\n\nBased on the summary result of the model we trained, the Out-of-Bag (OOB) error estimate is 25%, indicating that about 75% of observations were accurately predicted on average by the ensemble. The confusion matrix also shows class-specific errors: a 32% misclassification rate for class no purchase and a 20% misclassification rate for purchase.\nFor the AVPs, while are useful tools for linear models, they are not suitable for non-linear models like Random Forests. This is because AVPs are based on the assumption of linear additivity that they visualize the marginal effect of one predictor on the response after adjusting for others in a linear regression setting. Hence, we rely on the other two plots, PDP and LIME, to interpret non-linear models more appropriately.\n\n\n3.2.2 Partial Dependence Plots\nIn non-linear models, PDPs help us to better understand how each feature influences the projections of our non-linear model. Even when the model incorporates complex, non-linear interactions, this allows us to see the marginal influence of every variable.\n\n\nCode\n# Extract numeric variables from sim_data\nnum_vars &lt;- sim_data %&gt;%\n  select(where(is.numeric)) %&gt;%\n  colnames()\n\n# Prepare rug data for plotting\nrug_df &lt;- sim_data %&gt;%\n  select(all_of(num_vars)) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"variable\", values_to = \"x\")\n\n# Create PDPs for each numeric feature\npdp_list_rf &lt;- lapply(num_vars, function(var) {\n  pd &lt;- pdp::partial(\n    object = rf_model,\n    pred.var = var,\n    train = sim_data,\n    prob = TRUE\n  )\n  names(pd)[1] &lt;- \"x\"\n  pd$variable &lt;- var\n  pd\n})\n\n# Combine into single data frame\npdp_df_rf &lt;- bind_rows(pdp_list_rf)\n\n# Sort variables by standard deviation\nsd_order &lt;- pdp_df_rf %&gt;%\n  group_by(variable) %&gt;%\n  summarise(sd_yhat = sd(yhat)) %&gt;%\n  arrange(desc(sd_yhat))\n\n# Reorder for plotting\npdp_df_rf$variable &lt;- factor(as.character(pdp_df_rf$variable),\n                             levels = as.character(sd_order$variable))\nrug_df$variable &lt;- factor(as.character(rug_df$variable),\n                          levels = as.character(sd_order$variable))\n\n# Plot\nlibrary(ggplot2)\nggplot(pdp_df_rf, aes(x = x, y = yhat)) +\n  geom_line(color = \"blue\", linewidth = 0.7) +\n  geom_rug(data = rug_df, aes(x = x), color = \"red\", alpha = 0.2, inherit.aes = FALSE) +\n  facet_wrap(~ variable, scales = \"free_x\") +\n  labs(title = \"PDP – Random Forest\",\n       x = \"Feature Value\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nBased on the result from PDP plots, which we plot then in descending standard deviation order, we analysis their trends case by case.\ncategory_count: it shows a stong upward trend, suggesting that consumers who explore more product categories are considerably more likely to buy, probably because of more involvement or interest. browsing_time reveals an inverted U-shaped correlation, which means the probability of purchasing rises first but falls significantly after 2.5–3 hours, implying that prolonged browsing could indicate not purchase rather than buying. clicked_ad shows a negative linear trend as well, suggesting that people who click on advertising are really less likely to purchase. Likewise, items_in_cart has a significant negative slope that could suggest more items in the cart correspond to lower purchase probability. Lastly, previous_purchases presents a slight negative effect, suggesting that users with previous purchases show a slightly lower probability of purchasing again.\n\n\n3.2.3 LIME\nSimilar as for Linear model, we also perform the LIME to capture the important features for each case in our test set(the first three observations).\n\n\nCode\nset.seed(5293)\nmodel_type.randomForest &lt;- function(x, ...) 'classification'\n\n# Train Random Forest model\nrf_model &lt;- randomForest(purchase ~ ., data = train_data, ntree = 100)\n\nexplainer_rf &lt;- lime(\n  x = train_data %&gt;% select(-purchase),\n  model = rf_model,\n  bin_continuous = TRUE\n)\nexplanation_rf &lt;- explain(\n  x = test_data %&gt;% select(-purchase),\n  explainer = explainer_rf,\n  n_labels = 1,\n  n_features = 3  # top 3 features per instance\n)\nplot_features(explanation_rf)\n\n\n\n\n\n\n\n\n\n\nFor case 1: The model correctly predicted a non-purchase with 69% probability. Features like having 1 or less goods in the basket, a moderate browsing time (between 2.26 and 2.99), and a mid-range category count all helped to support this mainly since they all harmed the purchase probability.\nFor case 2: The model wrongly predicted a purchase with 85% probability. Two features (low category count and moderate items in cart) support the purchase prediction. Short browsing time slightly contradicts it, but overall, the model predicts purchase.\nFor case 3: The model correctly predicts purchase, with 76% probability, mainly due to low category count, but two features pull against that. The moderate fit suggests the explanation is reasonably reliable.\n\n\n\n3.2.4 Model Evaluation\nTo better understand the overall contribution of each feature to the prediction accuracy of the Random Forest model, we apply permutation importance. For each variables, we randomly shuffling its values and observing the changes in result in model performance. The idea is that if shuffling a feature substantially decreases the accuracy, that feature must be important for the model. For the robustness, we also calculate the \\(90%\\) confidence interval.\n\n\nCode\n# Set seed\nset.seed(5293)\n\n# Train the model\nrf_model &lt;- randomForest(as.factor(purchase) ~ ., data = sim_data)\n\n# Run permutation importance 30 times\nperm_list &lt;- replicate(\n  30,\n  vi_permute(\n    object = rf_model,\n    feature_names = setdiff(names(sim_data), \"purchase\"),\n    train = sim_data,\n    target = \"purchase\",\n    metric = \"accuracy\",\n    nsim = 1,\n    pred_wrapper = function(object, newdata) {\n      predict(object, newdata = newdata, type = \"response\")\n    }\n  ),\n  simplify = FALSE\n)\n\n# Combine results\nraw_long &lt;- bind_rows(perm_list, .id = \"rep\")\n\n# Compute 90% percentile-based CI\nci_bounds &lt;- raw_long %&gt;%\n  group_by(Variable) %&gt;%\n  summarise(\n    mean = mean(Importance),\n    lower = quantile(Importance, 0.05),\n    upper = quantile(Importance, 0.95),\n    .groups = \"drop\"\n  )\n\n\n\n\nCode\nggplot(raw_long, aes(x = Importance, y = reorder(Variable, Importance))) +\n  geom_jitter(size = 1, height = 0.3, alpha = 0.6) +\n  geom_errorbarh(\n    data = ci_bounds,\n    mapping = aes(xmin = lower, xmax = upper, y = reorder(Variable, mean)),\n    inherit.aes = FALSE, \n    height = 0.2,\n    color = \"red\",\n    linewidth = 0.8\n  ) +\n  labs(\n    title = \"Permutation Importance with 90% CI (Percentile Method)\",\n    x = \"Importance\",\n    y = \"Variable\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nBased on the importance plot with the \\(90%\\) as the error bar, we can gain the permutation importance scores. Specifically, category_count is the most influential variable, with the highest mean importance and a comparable narrow confidence interval, indicating it consistently contributes to predictive performance. Follow with browsing_time and items_in_cart, they also show strong importance, meaning the browsing time and the number of items in their cart are also important for predicting purchases. By contrast, previous_purchases is the least important, with lowest mean importance, meaning that their influence on the model’s predictions is relatively minor. The result from here somehow consist with the result from the AVP of the linear model and the pdp plots from the non-linear models. A more detailed comparsion and analysis will present in the comparsion section.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results of Linear and Non-linear Models</span>"
    ]
  }
]