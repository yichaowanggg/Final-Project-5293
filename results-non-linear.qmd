# Non Linear Model

Based on the result we got previously from the linear model, we now aim to further investigate some more flexible model. Although the linear model gave us a strong basis and let us understand the patterns and individual feature contributions, it is naturally constrained in its capacity to capture non-linear interactions between variables. Hence we trained a Random Forest mode, which build many decision trees and combining their results, to helps us to explore the interaction effects and they can capture complex, non-linear relationships and interactions between variables. Furthermore, Random Forest is still compatible with interpretation tools like as Permutation Importance, PDP, and LIME, hence allowing a consistent and understandable analysis across modelling techniques.
Before the modelling process, install all packages:
```{r, echo=TRUE, results='hide', message = FALSE, warning = FALSE}
# packages
library(ggplot2)
library(tidyverse)
library(broom)
library(dplyr)
library(tidyr)
library(pdp)
library(lime)
library(randomForest)
library(vip)
```

And load the dataset we simulated before:
```{r, echo=TRUE, results='hide'}
set.seed(5293)
n <- 500

# Generate the features for the dataset
browsing_time <- rnorm(n, mean = 3, sd = 1)               # hours
category_count <- rpois(n, lambda = 4)                    # integer count
clicked_ad <- rbinom(n, 1, prob = 0.35)                   # binary
items_in_cart <- rpois(n, lambda = 2)                     # integer count
device_type <- rbinom(n, 1, prob = 0.6)                   # 0 = desktop, 1 = mobile
previous_purchases <- rbinom(n, 1, prob = 0.25)           # binary
noise <- rnorm(n, 0, 0.5)

# Simulate outcome
logit <- -1 + 
  0.8 * browsing_time - 
  0.6 * category_count + 
  1.2 * clicked_ad + 
  0.5 * items_in_cart + 
  0.4 * previous_purchases +
  0.3 * device_type +
  noise

p <- 1 / (1 + exp(-logit))
purchase <- rbinom(n, 1, p)

# Assemble data frame
sim_data <- data.frame(
  browsing_time,
  category_count,
  clicked_ad,
  items_in_cart,
  device_type = factor(device_type, levels = c(0, 1), labels = c("desktop", "mobile")),
  previous_purchases,
  purchase
)
#head(sim_data)
```



## Fit the Random Forest Model

We trained the model on the same dataset as the linear models, `sim_data`, and fit the model by buliding 100 decision trees with randomly choose two variables at every split. To assist further interpretation, the model also monitors variable relevance.
```{r, message = FALSE, warning = FALSE}
# Fit the random forest model
rf_model <- randomForest(
  purchase ~ .,
  data = sim_data,
  ntree = 100,
  importance = TRUE
)

# View model summary
print(rf_model)
```
Based on the summary result of the model we trained, the Out-of-Bag (OOB) error estimate is 25%, indicating that about 75% of observations were accurately predicted on average by the ensemble. The confusion matrix also shows class-specific errors: a 32% misclassification rate for class no purchase and a 20% misclassification rate for purchase.


For the AVPs, while are useful tools for linear models, they are not suitable for non-linear models like Random Forests. This is because AVPs are based on the assumption of linear additivity that they visualize the marginal effect of one predictor on the response after adjusting for others in a linear regression setting. Hence, we rely on the other two plots, PDP and LIME, to interpret non-linear models more appropriately.

## Partial Dependence Plots
In non-linear models, PDPs help us to better understand how each feature influences the projections of our non-linear model. Even when the model incorporates complex, non-linear interactions, this allows us to see the marginal influence of every variable.
```{r, message = FALSE, warning = FALSE}
# Extract numeric variables from sim_data
num_vars <- sim_data %>%
  select(where(is.numeric)) %>%
  colnames()

# Prepare rug data for plotting
rug_df <- sim_data %>%
  select(all_of(num_vars)) %>%
  pivot_longer(cols = everything(), names_to = "variable", values_to = "x")

# Create PDPs for each numeric feature
pdp_list_rf <- lapply(num_vars, function(var) {
  pd <- pdp::partial(
    object = rf_model,
    pred.var = var,
    train = sim_data,
    prob = TRUE
  )
  names(pd)[1] <- "x"
  pd$variable <- var
  pd
})

# Combine into single data frame
pdp_df_rf <- bind_rows(pdp_list_rf)

# Sort variables by standard deviation
sd_order <- pdp_df_rf %>%
  group_by(variable) %>%
  summarise(sd_yhat = sd(yhat)) %>%
  arrange(desc(sd_yhat))

# Reorder for plotting
pdp_df_rf$variable <- factor(as.character(pdp_df_rf$variable),
                             levels = as.character(sd_order$variable))
rug_df$variable <- factor(as.character(rug_df$variable),
                          levels = as.character(sd_order$variable))

# Plot
library(ggplot2)
ggplot(pdp_df_rf, aes(x = x, y = yhat)) +
  geom_line(color = "blue", linewidth = 0.7) +
  geom_rug(data = rug_df, aes(x = x), color = "red", alpha = 0.2, inherit.aes = FALSE) +
  facet_wrap(~ variable, scales = "free_x") +
  labs(title = "PDP – Random Forest",
       x = "Feature Value") +
  theme_minimal()
```
Based on the result from PDP plots, which we plot then in descending standard deviation order, we analysis their trends case by case. 

`category_count`: it shows a stong upward trend, suggesting that consumers who explore more product categories are considerably more likely to buy, probably because of more involvement or interest. `browsing_time` reveals an inverted U-shaped correlation, which means the probability of purchasing rises first but falls significantly after 2.5–3 hours, implying that prolonged browsing could indicate not purchase rather than buying. `clicked_ad` shows a negative linear trend as well, suggesting that people who click on advertising are really less likely to purchase. Likewise, `items_in_cart` has a significant negative slope that could suggest more items in the cart correspond to lower purchase probability. Lastly, `previous_purchases` presents a slight negative effect, suggesting that users with previous purchases show a slightly lower probability of purchasing again. 



## LIME
```{r, message = FALSE, warning = FALSE}
set.seed(5293)
model_type.randomForest <- function(x, ...) 'classification'

# Train Random Forest model
rf_model <- randomForest(purchase ~ ., data = train_data, ntree = 100)

explainer_rf <- lime(
  x = train_data %>% select(-purchase),
  model = rf_model,
  bin_continuous = TRUE
)
explanation_rf <- explain(
  x = test_data %>% select(-purchase),
  explainer = explainer_rf,
  n_labels = 1,
  n_features = 3  # top 3 features per instance
)
plot_features(explanation_rf)

```





