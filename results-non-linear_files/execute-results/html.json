{
  "hash": "89bd20fa0f4a0de9c7df66611d9ef5c6",
  "result": {
    "engine": "knitr",
    "markdown": "# Non Linear Model\n\nBased on the result we got previously from the linear model, we now aim to further investigate some more flexible model. Although the linear model gave us a strong basis and let us understand the patterns and individual feature contributions, it is naturally constrained in its capacity to capture non-linear interactions between variables. Hence we trained a Random Forest mode, which build many decision trees and combining their results, to helps us to explore the interaction effects and they can capture complex, non-linear relationships and interactions between variables. Furthermore, Random Forest is still compatible with interpretation tools like as Permutation Importance, PDP, and LIME, hence allowing a consistent and understandable analysis across modelling techniques.\nBefore the modelling process, install all packages:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# packages\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(pdp)\nlibrary(lime)\nlibrary(randomForest)\nlibrary(vip)\n```\n:::\n\n\n\n\n\nAnd load the dataset we simulated before:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(5293)\nn <- 500\n\n# Generate the features for the dataset\nbrowsing_time <- rnorm(n, mean = 3, sd = 1)               # hours\ncategory_count <- rpois(n, lambda = 4)                    # integer count\nclicked_ad <- rbinom(n, 1, prob = 0.35)                   # binary\nitems_in_cart <- rpois(n, lambda = 2)                     # integer count\ndevice_type <- rbinom(n, 1, prob = 0.6)                   # 0 = desktop, 1 = mobile\nprevious_purchases <- rbinom(n, 1, prob = 0.25)           # binary\nnoise <- rnorm(n, 0, 0.5)\n\n# Simulate outcome\nlogit <- -1 + \n  0.8 * browsing_time - \n  0.6 * category_count + \n  1.2 * clicked_ad + \n  0.5 * items_in_cart + \n  0.4 * previous_purchases +\n  0.3 * device_type +\n  noise\n\np <- 1 / (1 + exp(-logit))\npurchase <- rbinom(n, 1, p)\n\n# Assemble data frame\nsim_data <- data.frame(\n  browsing_time,\n  category_count,\n  clicked_ad,\n  items_in_cart,\n  device_type = factor(device_type, levels = c(0, 1), labels = c(\"desktop\", \"mobile\")),\n  previous_purchases,\n  purchase\n)\n#head(sim_data)\n```\n:::\n\n\n\n\n\n## Fit the Random Forest Model\n\nWe trained the model on the same dataset as the linear models, `sim_data`, and fit the model by buliding 100 decision trees with randomly choose two variables at every split. To assist further interpretation, the model also monitors variable relevance.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the random forest model\nrf_model <- randomForest(\n  purchase ~ .,\n  data = sim_data,\n  ntree = 100,\n  importance = TRUE\n)\n\n# View model summary\nprint(rf_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\n randomForest(formula = purchase ~ ., data = sim_data, ntree = 100,      importance = TRUE) \n               Type of random forest: regression\n                     Number of trees: 100\nNo. of variables tried at each split: 2\n\n          Mean of squared residuals: 0.1628973\n                    % Var explained: 33.54\n```\n\n\n:::\n:::\n\n\n\n\nBased on the summary result of the model we trained, the Out-of-Bag (OOB) error estimate is 25%, indicating that about 75% of observations were accurately predicted on average by the ensemble. The confusion matrix also shows class-specific errors: a 32% misclassification rate for class no purchase and a 20% misclassification rate for purchase.\n\n\nFor the AVPs, while are useful tools for linear models, they are not suitable for non-linear models like Random Forests. This is because AVPs are based on the assumption of linear additivity that they visualize the marginal effect of one predictor on the response after adjusting for others in a linear regression setting. Hence, we rely on the other two plots, PDP and LIME, to interpret non-linear models more appropriately.\n\n## Partial Dependence Plots\nIn non-linear models, PDPs help us to better understand how each feature influences the projections of our non-linear model. Even when the model incorporates complex, non-linear interactions, this allows us to see the marginal influence of every variable.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract numeric variables from sim_data\nnum_vars <- sim_data %>%\n  select(where(is.numeric)) %>%\n  colnames()\n\n# Prepare rug data for plotting\nrug_df <- sim_data %>%\n  select(all_of(num_vars)) %>%\n  pivot_longer(cols = everything(), names_to = \"variable\", values_to = \"x\")\n\n# Create PDPs for each numeric feature\npdp_list_rf <- lapply(num_vars, function(var) {\n  pd <- pdp::partial(\n    object = rf_model,\n    pred.var = var,\n    train = sim_data,\n    prob = TRUE\n  )\n  names(pd)[1] <- \"x\"\n  pd$variable <- var\n  pd\n})\n\n# Combine into single data frame\npdp_df_rf <- bind_rows(pdp_list_rf)\n\n# Sort variables by standard deviation\nsd_order <- pdp_df_rf %>%\n  group_by(variable) %>%\n  summarise(sd_yhat = sd(yhat)) %>%\n  arrange(desc(sd_yhat))\n\n# Reorder for plotting\npdp_df_rf$variable <- factor(as.character(pdp_df_rf$variable),\n                             levels = as.character(sd_order$variable))\nrug_df$variable <- factor(as.character(rug_df$variable),\n                          levels = as.character(sd_order$variable))\n\n# Plot\nlibrary(ggplot2)\nggplot(pdp_df_rf, aes(x = x, y = yhat)) +\n  geom_line(color = \"blue\", linewidth = 0.7) +\n  geom_rug(data = rug_df, aes(x = x), color = \"red\", alpha = 0.2, inherit.aes = FALSE) +\n  facet_wrap(~ variable, scales = \"free_x\") +\n  labs(title = \"PDP – Random Forest\",\n       x = \"Feature Value\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](results-non-linear_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\nBased on the result from PDP plots, which we plot then in descending standard deviation order, we analysis their trends case by case. \n\n`category_count`: it shows a stong upward trend, suggesting that consumers who explore more product categories are considerably more likely to buy, probably because of more involvement or interest. `browsing_time` reveals an inverted U-shaped correlation, which means the probability of purchasing rises first but falls significantly after 2.5–3 hours, implying that prolonged browsing could indicate not purchase rather than buying. `clicked_ad` shows a negative linear trend as well, suggesting that people who click on advertising are really less likely to purchase. Likewise, `items_in_cart` has a significant negative slope that could suggest more items in the cart correspond to lower purchase probability. Lastly, `previous_purchases` presents a slight negative effect, suggesting that users with previous purchases show a slightly lower probability of purchasing again. \n\n\n\n\n\n\n\n\n\n",
    "supporting": [
      "results-non-linear_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}